@inproceedings{jiang-etal-2023-low,
    title = "{``}Low-Resource{''} Text Classification: A Parameter-Free Classification Method with Compressors",
    author = "Jiang, Zhiying  and
      Yang, Matthew  and
      Tsirlin, Mikhail  and
      Tang, Raphael  and
      Dai, Yiqin  and
      Lin, Jimmy",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.426",
    doi = "10.18653/v1/2023.findings-acl.426",
    pages = "6810--6828",
    abstract = "Deep neural networks (DNNs) are often used for text classification due to their high accuracy. However, DNNs can be computationally intensive, requiring millions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize, and to transfer to out-of-distribution (OOD) cases in practice. In this paper, we propose a non-parametric alternative to DNNs that{'}s easy, lightweight, and universal in text classification: a combination of a simple compressor like \textit{gzip} with a $k$-nearest-neighbor classifier. Without any training parameters, our method achieves results that are competitive with non-pretrained deep learning methods on six in-distribution datasets.It even outperforms BERT on all five OOD datasets, including four low-resource languages. Our method also excels in the few-shot setting, where labeled data are too scarce to train DNNs effectively.",
}

@misc{eldan2023tinystories,
      title={TinyStories: How Small Can Language Models Be and Still Speak Coherent English?}, 
      author={Ronen Eldan and Yuanzhi Li},
      year={2023},
      eprint={2305.07759},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@online{kvikio,
  author = {rapidsai},
  title = {kvikio},
  howpublished = {\url{https://github.com/rapidsai/kvikio}},
  note = {Accessed on January 10, 2023},
}

@misc{nvcomp,
  author = {{nvCOMP} Development Team},
  title = {nvCOMP: GPU-Accelerated Compression Libraries},
  howpublished = {\url{https://developer.nvidia.com/nvcomp}},
  note = {Accessed on January 10, 2023},
}

@misc{cupy_project,
  author = {{CuPy} Development Team},
  title = {CuPy: A NumPy-like API for GPU-accelerated Computing with Python},
  howpublished = {\url{https://cupy.dev}},
  note = {Accessed on January 10, 2023},
}

@misc{rapidsai,
  author = {{NVIDIA} RAPIDS AI Team},
  title = {RAPIDS: Data Science Acceleration Libraries},
  howpublished = {\url{https://rapids.ai}},
  note = {Accessed on January 1, 2023},
}

@misc{kudo2018sentencepiece,
      title={SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing}, 
      author={Taku Kudo and John Richardson},
      year={2018},
      eprint={1808.06226},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{karpathy2021lets,
  author = {Karpathy, Andrej},
  title = {Let's build GPT: from scratch, in code, spelled out},
  howpublished = {\url{https://www.youtube.com/watch?v=kCc8FmEb1nY}},
  note = {Accessed on January 1, 2023},
  publisher = {YouTube},
  month = {November},
  year = {2021},
  videorecorder = {Karpathy, Andrej},
}